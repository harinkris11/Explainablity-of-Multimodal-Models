{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d602385c-baf5-4fad-a730-60624b7f3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecaa036-bb7b-45fc-931b-dfb0d46317c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer.tokenize(\"Type Dog | Age 1 | Breed1 Mixed_Breed | Breed2 nan | Gender: Male | Color1: Black | Color2: NA | Color3: NA | MaturitySize: Medium | FurLength: Short | Vaccinated: No | Dewormed: No | Sterilized: No | Health: Healthy | Quantity: 1 | Fee: 0 | StateName: Selangor | VideoAmt: 0 | PhotoAmt: 3.0 | Description: This handsome yet cute boy is up for adoption. He is the most playful pal we've seen in our puppies. He loves to nibble on shoelaces , Chase you at such a young age. Imagine what a cute brat he will be when he grows. We are looking for a loving home for Hunter , one that will take care of him and give him the love that he needs. Please call urgently if you would like to adopt this cutie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a01c1a3f-6164-49c2-bfd2-7d9de8059165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc168a82-92f6-4abf-ab27-6da93dce00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(\"Type: Dog | Age: 1 | Breed1: Mixed Breed | Breed2: nan | Gender: Male | Color1: Black | Color2: NA | Color3: NA | MaturitySize: Medium | FurLength: Short | Vaccinated: No | Dewormed: No | Sterilized: No | Health: Healthy | Quantity: 1 | Fee: 0 | StateName: Selangor | VideoAmt: 0 | PhotoAmt: 3.0 | Description: This handsome yet cute boy is up for adoption. He is the most playful pal we've seen in our puppies. He loves to nibble on shoelaces , Chase you at such a young age. Imagine what a cute brat he will be when he grows. We are looking for a loving home for Hunter , one that will take care of him and give him the love that he needs. Please call urgently if you would like to adopt this cutie.\",\n",
    "                           add_special_tokens=True,\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=256,\n",
    "                           return_token_type_ids=False,\n",
    "                           return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7833b4eb-c99d-41d0-a152-988c6d8d27f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
       "array([[  101,  2828,  1024,  3899,  1064,  2287,  1024,  1015,  1064,\n",
       "         8843,  2487,  1024,  3816,  8843,  1064,  8843,  2475,  1024,\n",
       "        16660,  1064,  5907,  1024,  3287,  1064,  3609,  2487,  1024,\n",
       "         2304,  1064,  3609,  2475,  1024,  6583,  1064,  3609,  2509,\n",
       "         1024,  6583,  1064, 16736,  5332,  4371,  1024,  5396,  1064,\n",
       "         6519,  7770, 13512,  2232,  1024,  2460,  1064, 12436, 14693,\n",
       "        23854,  1024,  2053,  1064, 24903,  2953,  7583,  1024,  2053,\n",
       "         1064, 26261, 15928,  3550,  1024,  2053,  1064,  2740,  1024,\n",
       "         7965,  1064, 11712,  1024,  1015,  1064,  7408,  1024,  1014,\n",
       "         1064, 24161, 14074,  1024, 28904,  1064,  2678,  3286,  2102,\n",
       "         1024,  1014,  1064,  6302,  3286,  2102,  1024,  1017,  1012,\n",
       "         1014,  1064,  6412,  1024,  2023,  8502,  2664, 10140,  2879,\n",
       "         2003,  2039,  2005,  9886,  1012,  2002,  2003,  1996,  2087,\n",
       "        18378, 14412,  2057,  1005,  2310,  2464,  1999,  2256, 26781,\n",
       "        13046,  1012,  2002,  7459,  2000,  9152, 11362,  2006, 10818,\n",
       "        19217,  2015,  1010,  5252,  2017,  2012,  2107,  1037,  2402,\n",
       "         2287,  1012,  5674,  2054,  1037, 10140, 28557,  2002,  2097,\n",
       "         2022,  2043,  2002,  7502,  1012,  2057,  2024,  2559,  2005,\n",
       "         1037,  8295,  2188,  2005,  4477,  1010,  2028,  2008,  2097,\n",
       "         2202,  2729,  1997,  2032,  1998,  2507,  2032,  1996,  2293,\n",
       "         2008,  2002,  3791,  1012,  3531,  2655, 25478,  2065,  2017,\n",
       "         2052,  2066,  2000, 11092,  2023,  3013,  2666,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2439317-a2bc-4a81-b705-18232c485a72",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 71) (run_shap.py, line 71)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3526\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\n\u001b[0;31m    import Masker_Model.TextNTabularExplanations.src.run_shap\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/cephfs/DSC261/project/Masker_Model/TextNTabularExplanations/src/run_shap.py:71\u001b[0;36m\u001b[0m\n\u001b[0;31m    return_token_type_ids= False, return_tensors = 'tf)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 71)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('ceph/DSC261/project/Masker_Model/TextNTabularExplanations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94caec3-4473-415c-8be8-636a39382adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c6748a-d92d-4c38-b8a9-6e3707578ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.14\n",
      "  Obtaining dependency information for tensorflow==2.14 from https://files.pythonhosted.org/packages/09/63/25e76075081ea98ec48f23929cefee58be0b42212e38074a9ec5c19e838c/tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (16.0.6)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/87/91/d57c2d22e4801edeb7f3e7939214c0ea8a28c6e16f85208c2df2145e0213/ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (4.23.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (4.5.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14)\n",
      "  Obtaining dependency information for wrapt<1.15,>=1.11.0 from https://files.pythonhosted.org/packages/7f/1b/e0439eec0db6520968c751bc7e12480bb80bb8d939190e0e55ed762f3c7a/wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.14) (1.59.0)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.14) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.23.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.2.2)\n",
      "Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.15.0\n",
      "    Uninstalling wrapt-1.15.0:\n",
      "      Successfully uninstalled wrapt-1.15.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.13.0\n",
      "    Uninstalling tensorflow-estimator-2.13.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.0\n",
      "    Uninstalling tensorflow-2.13.0:\n",
      "      Successfully uninstalled tensorflow-2.13.0\n",
      "Successfully installed keras-2.14.0 ml-dtypes-0.2.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ce637-e582-4b73-8235-8d44bede17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import src.run_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65189d91-5ddf-47a2-bdbf-aee2fc573cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534145f2-2355-401a-9e83-f2fe437796f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3482c07-04c9-4983-b3ff-c762e3cc3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/cephfs/DSC261/project/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1644101-4cfd-4abd-8c3a-4dba2afde714",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_data.drop(['Name', 'RescuerID'], axis = 1, inplace = True)\n",
    "train_data['AdoptionSpeed'][train_data['AdoptionSpeed']<=2] = 1\n",
    "train_data['AdoptionSpeed'][train_data['AdoptionSpeed']>2] = 0\n",
    "tr_data = train_data.copy(deep=True)\n",
    "tr_data.dropna(inplace = True)\n",
    "tr_data.reset_index(inplace = True)\n",
    "train_text = tr_data[['Description','AdoptionSpeed']]\n",
    "\n",
    "img_files = os.listdir('/cephfs/DSC261/project/train_images/')\n",
    "files = [img for img in img_files if img.endswith('-1.jpg')]\n",
    "img_names = set(imgname[:-6] for imgname in files)\n",
    "img_omit = set(tr_data['PetID']) - img_names\n",
    "tr_data = tr_data[~tr_data.PetID.isin(img_omit)]\n",
    "tr_img = tr_data[['PetID','AdoptionSpeed']]\n",
    "def generate_image_name(pet_id):\n",
    "    return f\"{pet_id}-1.jpg\"\n",
    "tr_img['ImageName'] = tr_img['PetID'].apply(generate_image_name)\n",
    "\n",
    "tr_text = tr_data[['Description', 'AdoptionSpeed']]\n",
    "\n",
    "tr_tab = tr_data.drop(['Description', 'PetID', 'index' ,'Breed2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf9492-0486-453a-87f0-1b27061f5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_df = pd.read_csv('/cephfs/DSC261/project/PetFinder-BreedLabels.csv')\n",
    "breed_df.drop(['Type'], axis = 1, inplace = True)\n",
    "merged_data = tr_tab.merge(breed_df, left_on = 'Breed1', right_on = 'BreedID', how = 'left').drop(\n",
    "    ['Breed1', 'BreedID'], axis = 1)\n",
    "breed_counts_x = merged_data['BreedName'].value_counts()\n",
    "replace_breeds_x = breed_counts_x[breed_counts_x <= 100].index\n",
    "merged_data['BreedName'] = merged_data['BreedName'].apply(lambda x: 'Others' if x in replace_breeds_x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725f6a4-2a00-45f0-9574-f046e36afaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = pd.read_csv('/cephfs/DSC261/project/state_labels.csv')\n",
    "merged_data = merged_data.merge(state_df, how = 'left', left_on = 'State', \n",
    "                                right_on = 'StateID').drop(['StateID', 'State'], axis = 1)\n",
    "states = merged_data.StateName.value_counts()\n",
    "statelst = states[states<150].index.tolist()\n",
    "merged_data['BreedName'] = merged_data['BreedName'].apply(lambda x: 'Others' if x in replace_breeds_x else x)\n",
    "merged_data['StateName'] = merged_data['StateName'].apply(lambda x: 'Others' if x in statelst else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75d67c-0793-44b4-807b-3b300951a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(merged_data[['Type', 'Gender', 'Vaccinated', 'Dewormed', 'Sterilized', 'BreedName', 'StateName']])\n",
    "\n",
    "for color_col in ['Color1', 'Color2', 'Color3']:\n",
    "    color_dummies = pd.get_dummies(merged_data[color_col], prefix='Color')\n",
    "    if color_col == 'Color1':\n",
    "        factor = 3\n",
    "    elif color_col == 'Color2': \n",
    "        factor = 2\n",
    "    else: factor = 1\n",
    "    data_dummies = data_dummies.add(color_dummies.multiply(factor), fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a14038-c440-447e-92ec-97c20196b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tab_data = pd.concat([merged_data, data_dummies], axis = 1)\n",
    "final_tab_data.drop(['Type', 'Gender', 'Vaccinated', 'Dewormed', 'Sterilized', \n",
    "                      'BreedName', 'StateName', 'Color1', 'Color2', 'Color3'], axis =1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f2749-4bae-447e-aca3-c9117f26e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a4ba9-25f0-469e-9822-524882c3e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_df = pd.read_csv('/cephfs/DSC261/project/PetFinder-BreedLabels.csv')\n",
    "breed_df.drop(['Type'], axis = 1, inplace = True)\n",
    "merged_data = tr_data.merge(breed_df, left_on = 'Breed1', right_on = 'BreedID', how = 'left').drop(\n",
    "    ['Breed1', 'BreedID'], axis = 1)\n",
    "merged_data.rename({'BreedName': 'Breed1'}, axis =1, inplace = True)\n",
    "merged_data = merged_data.merge(breed_df, left_on = 'Breed2', right_on = 'BreedID', how = 'left').drop(\n",
    "    ['Breed2', 'BreedID'], axis = 1)\n",
    "merged_data.rename({'BreedName': 'Breed2'}, axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7533c-17b3-4b83-b9b2-8bfc8f470e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping_type = {1: 'Dog', 2: 'Cat'}\n",
    "merged_data['Type'] = merged_data['Type'].map(type_mapping_type)\n",
    "\n",
    "type_mapping_gender = {1: 'Male', 2: 'Female', 3: 'Mixed'}\n",
    "merged_data['Gender'] = merged_data['Gender'].map(type_mapping_gender)\n",
    "\n",
    "type_mapping_color = {1: 'Black', 2: 'Brown', 3: 'Golden', 4: 'Yellow', 5: 'Cream', 6: 'Gray', 7: 'White', 0: 'NA' }\n",
    "merged_data['Color1'] = merged_data['Color1'].map(type_mapping_color)\n",
    "merged_data['Color2'] = merged_data['Color2'].map(type_mapping_color)\n",
    "merged_data['Color3'] = merged_data['Color3'].map(type_mapping_color)\n",
    "\n",
    "type_mapping_vaccination = {1: 'Yes', 2: 'No', 3: 'Not Sure'}\n",
    "merged_data['Vaccinated'] = merged_data['Vaccinated'].map(type_mapping_vaccination)\n",
    "\n",
    "type_mapping_Dewormed = {1: 'Yes', 2: 'No', 3: 'Not Sure'}\n",
    "merged_data['Dewormed'] = merged_data['Dewormed'].map(type_mapping_Dewormed)\n",
    "\n",
    "type_mapping_Sterilized = {1: 'Yes', 2: 'No', 3: 'Not Sure'}\n",
    "merged_data['Sterilized'] = merged_data['Sterilized'].map(type_mapping_Sterilized)\n",
    "\n",
    "#new\n",
    "type_mapping_MaturitySize = {1: 'Small', 2: 'Medium', 3: 'Large', 4: 'Extra Large', 0: 'Not Specified'}\n",
    "merged_data['MaturitySize'] = merged_data['MaturitySize'].map(type_mapping_MaturitySize)\n",
    "\n",
    "type_mapping_FurLength = {1: 'Short', 2: 'Medium', 3: 'Long', 0: 'Not Specified'}\n",
    "merged_data['FurLength'] = merged_data['FurLength'].map(type_mapping_FurLength)\n",
    "\n",
    "type_mapping_Health = {1: 'Healthy', 2: 'Minor Injury', 3: 'Serious Injury', 0: 'Not Specified'}\n",
    "merged_data['Health'] = merged_data['Health'].map(type_mapping_Health)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb6686-5999-4deb-874b-fbc7f97d363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = pd.read_csv('/cephfs/DSC261/project/state_labels.csv')\n",
    "merged_data = merged_data.merge(state_df, how = 'left', left_on = 'State', \n",
    "                                right_on = 'StateID').drop(['StateID', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217298ae-797f-4c08-a220-5861dc19a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n",
    "       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n",
    "       'Sterilized', 'Health', 'Quantity', 'Fee', 'StateName', 'VideoAmt',\n",
    "         'PhotoAmt','AdoptionSpeed', 'Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1efeb1-bb70-4c56-a8cf-9cdd07c064ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8bfc5-ad3c-4385-b543-d32dedbb1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a31b41-b27a-477c-b882-94c868c50ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37b4636-9ca7-40fb-8105-f01495863ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src.run_shap.run_shap(merged_data, test_set_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57156717-149d-419d-be19-42617a39b7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a56e8-c61d-40ff-acdf-9c213ae3f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from src.utils import legacy_get_dataset_info\n",
    "from transformers import pipeline, AutoTokenizer, TFBertModel\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.utils import token_segments, text_ft_index_ends, format_text_pred, ConfigLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import BertTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# from src.models import Model\n",
    "from src.models import AllAsTextModel\n",
    "# from src.joint_masker import JointMasker\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eabe000a-cdd1-4dc1-bbc5-522d52a91973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred) \n",
    "    f1 = 2 * tf.reduce_sum(y_true * y_pred) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1e-16)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def run_shap(merged_data, max_samples=100, test_set_size=100):\n",
    "    all_text_versions = [\"all_as_text\"]\n",
    "    train_df, test_df = train_test_split(merged_data, random_state = 42, test_size = 0.25)\n",
    "    y_train = train_df['AdoptionSpeed']\n",
    "    test_df = test_df.sample(test_set_size, random_state=11)\n",
    "\n",
    "    model1 = load_model(\"/cephfs/DSC261/project/model/best_bert_unimodal.hdf5\", custom_objects={'f1_metric': f1_metric, 'TFBertModel': TFBertModel})\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased')  \n",
    "    model1.config = config\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, max_length = 256,\n",
    "                                             padding = 'max_length', truncation = True, \n",
    "                                              return_token_type_ids= False, return_tensors = 'tf')\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model1,\n",
    "        tokenizer=tokenizer,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        top_k=None,\n",
    "    )\n",
    "    cols_to_str_fn = lambda array: \" | \".join(\n",
    "        [f\"{col} {str(val)}\" for col, val in zip(train_df[['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'Description']].columns, array)])\n",
    "\n",
    "    model = AllAsTextModel(\n",
    "        text_pipeline=text_pipeline,\n",
    "        cols_to_str_fn=cols_to_str_fn,\n",
    "    )\n",
    "\n",
    "    np.random.seed(11)\n",
    "    x = test_df[['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'Description']].values\n",
    "    ord_train_df = train_df.copy()\n",
    "\n",
    "    tab_pt = sp.cluster.hierarchy.complete(\n",
    "        sp.spatial.distance.pdist(\n",
    "            ord_train_df[['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt']]\n",
    "            .values.T,\n",
    "            metric=\"correlation\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    masker = JointMasker(\n",
    "        tab_df=train_df[['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt']],\n",
    "        text_cols=train_df['Description'],\n",
    "        cols_to_str_fn=cols_to_str_fn,\n",
    "        tokenizer=tokenizer,\n",
    "        collapse_mask_token=True,\n",
    "        max_samples=max_samples,\n",
    "        tab_partition_tree=tab_pt,\n",
    "    )\n",
    "\n",
    "    explainer = shap.explainers.Partition(model=model.predict, masker=masker)\n",
    "    # print(x)\n",
    "    shap_vals = explainer(x)\n",
    "\n",
    "    output_dir = \"./model/shap_vals/\"\n",
    "    print(f\"Results will be saved @: {output_dir}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    with open(os.path.join(output_dir, f\"pet_bert_alltext_shap.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_vals, f)\n",
    "\n",
    "    return shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d165f2d-cdca-4cb9-8796-29178c07eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_text_baseline_shap(\n",
    "    merged_data,\n",
    "    # config_type,\n",
    "    test_set_size=100,\n",
    "):\n",
    "    train_df, test_df = train_test_split(merged_data, random_state = 42, test_size = 0.25)\n",
    "    y_train = train_df['AdoptionSpeed']\n",
    "    test_df = test_df.sample(test_set_size, random_state=11)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, max_length = 256,\n",
    "                                             padding = 'max_length', truncation = True, \n",
    "                                              return_token_type_ids= False, return_tensors = 'tf')\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model1,\n",
    "        tokenizer=tokenizer,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        top_k=None,\n",
    "    )\n",
    "\n",
    "    def cols_to_str_fn(array):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                f\"{col} {val}\"\n",
    "                for col, val in zip(\n",
    "                    train_df.drop('AdoptionSpeed', axis = 1).columns, array\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    np.random.seed(1)\n",
    "    x = list(\n",
    "        map(\n",
    "            cols_to_str_fn,\n",
    "            test_df.drop('AdoptionSpeed', axis = 1).values,\n",
    "        )\n",
    "    )\n",
    "    explainer = shap.Explainer(text_pipeline, tokenizer)\n",
    "    shap_vals = explainer(x)\n",
    "\n",
    "    output_dir = \"./models/shap_vals/\"\n",
    "    print(f\"Results will be saved @: {output_dir}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    with open(os.path.join(output_dir, f\"pet_bert_alltext_shap_baseline.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_vals, f)\n",
    "\n",
    "    return shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0861b7c4-d21d-4879-85e1-de6d87d2d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_summary_shap_vals(merged_data, ctype = 'pet_bert_alltext_shap',add_parent_dir=False): #config_type\n",
    "\n",
    "    with open(f\"./models/shap_vals/{ctype}.pkl\", \"rb\") as f:\n",
    "        shap_vals = pickle.load(f)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, max_length = 256,\n",
    "                                             padding = 'max_length', truncation = True, \n",
    "                                              return_token_type_ids= False, return_tensors = 'tf')\n",
    "\n",
    "    filepath = f\"./models/shap_vals/summed_{ctype}.pkl\"\n",
    "    print(\n",
    "        f\"\"\"\n",
    "            #################\n",
    "            {ctype}\n",
    "            #################\n",
    "            \"\"\"\n",
    "    )\n",
    "    if \"baseline\" not in ctype:\n",
    "        grouped_shap_vals = []\n",
    "        for label in range(2): #no. of labels\n",
    "            shap_for_label = []\n",
    "            for idx in tqdm(range(len(shap_vals))):\n",
    "                sv = shap_vals[idx, :, label]\n",
    "                text_ft_ends = text_ft_index_ends(\n",
    "                    sv.data[-1:], tokenizer\n",
    "                )\n",
    "                text_ft_ends = [len(merged_data.columns)-2] + [\n",
    "                    x + len(merged_data.columns)-2 + 1\n",
    "                    for x in text_ft_ends\n",
    "                ]\n",
    "                val = np.append(\n",
    "                    sv.values[: -1],\n",
    "                    [\n",
    "                        np.sum(sv.values[text_ft_ends[i] : text_ft_ends[i + 1]])\n",
    "                        for i in range(len(text_ft_ends) - 1)\n",
    "                    ]\n",
    "                    + [np.sum(sv.values[text_ft_ends[-1] :])],\n",
    "                )\n",
    "\n",
    "                shap_for_label.append(val)\n",
    "            grouped_shap_vals.append(np.vstack(shap_for_label))\n",
    "        print(f\"Saving to {filepath}\")\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_shap_vals), f)\n",
    "\n",
    "    else:\n",
    "        col_name_filepath = f\"./models/shap_vals/summed_{ctype}_col_names.pkl\"\n",
    "        colon_filepath = f\"./models/shap_vals/summed_{ctype}_colons.pkl\"\n",
    "        grouped_shap_vals = []\n",
    "        grouped_col_name_shap_vals = []\n",
    "        grouped_colon_shap_vals = []\n",
    "        for label in range(2):\n",
    "            shap_for_label = []\n",
    "            shap_for_col_name = []\n",
    "            shap_for_colon = []\n",
    "            for idx in tqdm(range(len(shap_vals))):\n",
    "                sv = shap_vals[idx, :, label]\n",
    "                stripped_data = np.array([item.strip() for item in sv.data])\n",
    "                text_ft_ends = (\n",
    "                    [1] + list(np.where(stripped_data == \"|\")[0]) + [len(sv.data) + 1]\n",
    "                )\n",
    "                if (\n",
    "                    len(text_ft_ends) != len(merged_data.columns)\n",
    "                ):\n",
    "                    text_ft_ends = (\n",
    "                        [1]\n",
    "                        + [\n",
    "                            i\n",
    "                            for i in list(np.where(stripped_data == \"|\")[0])\n",
    "                            if sv.data[i + 1].strip()\n",
    "                            in [\n",
    "                                token_segments(col, tokenizer)[0][1].strip()\n",
    "                                for col in merged_data.drop('AdoptionSpeed', axis = 1).columns\n",
    "                            ]\n",
    "                            + merged_data.drop('AdoptionSpeed', axis = 1).columns\n",
    "                            # + di.categorical_cols\n",
    "                            # + di.numerical_cols\n",
    "                            # + di.text_cols\n",
    "                        ]\n",
    "                        + [len(sv.data) + 1]\n",
    "                    )\n",
    "                assert (\n",
    "                    len(text_ft_ends)\n",
    "                    == len(merged_data.drop('AdoptionSpeed', axis = 1).columns) + 1\n",
    "                )\n",
    "                val = np.array(\n",
    "                    [\n",
    "                        np.sum(sv.values[text_ft_ends[i] : text_ft_ends[i + 1]])\n",
    "                        for i in range(len(text_ft_ends) - 1)\n",
    "                    ]\n",
    "                )\n",
    "                colon_idxs = np.where(stripped_data == \":\")[0]\n",
    "                col_idxs_after_ft = [\n",
    "                    colon_idxs[list(np.where(colon_idxs > te)[0])[0]]\n",
    "                    for te in text_ft_ends[:-1]\n",
    "                ]\n",
    "                ft_name_vals = np.array(\n",
    "                    [\n",
    "                        np.sum(sv.values[text_ft_ends[i] : col_idxs_after_ft[i]])\n",
    "                        for i in range(len(text_ft_ends) - 1)\n",
    "                    ]\n",
    "                )\n",
    "                colon_vals = np.array(sv.values[col_idxs_after_ft])\n",
    "                shap_for_label.append(val)\n",
    "                shap_for_col_name.append(ft_name_vals)\n",
    "                shap_for_colon.append(colon_vals)\n",
    "            grouped_shap_vals.append(np.vstack(shap_for_label))\n",
    "            grouped_col_name_shap_vals.append(shap_for_col_name)\n",
    "            grouped_colon_shap_vals.append(shap_for_colon)\n",
    "        print(f\"Saving to {filepath}\")\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_shap_vals), f)\n",
    "        print(f\"Saving to {col_name_filepath}\")\n",
    "        with open(col_name_filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_col_name_shap_vals), f)\n",
    "        print(f\"Saving to {colon_filepath}\")\n",
    "        with open(colon_filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_colon_shap_vals), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f145f4e-91d9-4809-acc2-4fb7635fc87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dfca6-b4c7-4467-b69e-e3764eebafea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7166d433-1ae4-40d9-b618-e9c969f34adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'Functional' is not supported for text-classification. Supported models are ['TFAlbertForSequenceClassification', 'TFBartForSequenceClassification', 'TFBertForSequenceClassification', 'TFCamembertForSequenceClassification', 'TFConvBertForSequenceClassification', 'TFCTRLForSequenceClassification', 'TFDebertaForSequenceClassification', 'TFDebertaV2ForSequenceClassification', 'TFDistilBertForSequenceClassification', 'TFElectraForSequenceClassification', 'TFEsmForSequenceClassification', 'TFFlaubertForSequenceClassification', 'TFFunnelForSequenceClassification', 'TFGPT2ForSequenceClassification', 'TFGPT2ForSequenceClassification', 'TFGPTJForSequenceClassification', 'TFLayoutLMForSequenceClassification', 'TFLayoutLMv3ForSequenceClassification', 'TFLongformerForSequenceClassification', 'TFMobileBertForSequenceClassification', 'TFMPNetForSequenceClassification', 'TFOpenAIGPTForSequenceClassification', 'TFRemBertForSequenceClassification', 'TFRobertaForSequenceClassification', 'TFRobertaPreLayerNormForSequenceClassification', 'TFRoFormerForSequenceClassification', 'TFTapasForSequenceClassification', 'TFTransfoXLForSequenceClassification', 'TFXLMForSequenceClassification', 'TFXLMRobertaForSequenceClassification', 'TFXLNetForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123 1 0 0 3.0\n",
      "  'This pet has been abandon at a veterinary clinic. Tame and like to sleep a lot.']\n",
      " [3 1 0 0 5.0\n",
      "  'Thank you for all who have contacted us. We found a good home with big space and a playmate for Teddy nearby our home. We get to visit him during the weekends.']\n",
      " [2 1 0 1 5.0\n",
      "  \"Mochi was found abandon in a little box near my office. She has little white socks and she likes to be cuddled! She's a mischievous, playful yet adorable kitty! Unfortunately the amount of cats (10) we're having and living in a condominium can't allow us to keep her for long term... hope to find her a home with full of love and care for her! Kindly contact me for more info!\"]\n",
      " ...\n",
      " [1 2 0 0 7.0\n",
      "  \"Venti has an extra toe on each hind leg, hence the name! (Yes, did you know that dogs have 5 toes on each fore leg but only 4 on each hind leg?) She was covered in fleas when we first found her, poor thing. Another puppy was found beside her, but it was already dead with its head buried in the soil. Vivo was found with a bite wound on her back, infected with maggots. One of her siblings was found lying nearby with 3 gaping holes in its neck, full of maggots and near death. We were unable to save it, and it died soon after. Vivo however, has since been given veterinary treatment and the wound has healed. They are both happy, active and playful puppies with huge appetites! If you'd like to give them a home, please call/text (ANG)\"]\n",
      " [1 1 150 1 13.0\n",
      "  'This puppy was rescued from the streets as his mummy was to weak to feed him. As a result, he was in a weak condition. We sent him to the vet for treatment and now he is a handsome and strong boy looking for a new LOVING home. Remark: 1. Adoption fee RM is for spaying fee in the future. Please call John if interested to view the puppies or thanks']\n",
      " [24 1 0 0 3.0\n",
      "  'Please adopt him!!! He need a home urgently. Location at jalan keris taman sri tebrau JB.']]\n",
      "(array(['[MASK]'], dtype='<U6'),)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34983/933974078.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"baseline\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrun_all_text_baseline_shap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrun_shap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgen_summary_shap_vals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34983/3464988309.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(merged_data, max_samples, test_set_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mshap_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./model/shap_vals/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results will be saved @: {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m     def __call__(self, *args, max_evals=500, fixed_context=None, main_effects=False, error_bounds=False, batch_size=\"auto\",\n\u001b[1;32m    126\u001b[0m                  outputs=None, silent=False):\n\u001b[1;32m    127\u001b[0m         \"\"\" Explain the output of the model on the given arguments.\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0merror_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"feature_names\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    268\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mm00\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# if not fixed background or no base value assigned then compute base value for a row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_base_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fixed_background\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_base_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the zero index param tells the masked model what the baseline is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mf11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mm00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0m_convert_delta_mask_to_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdo_delta_masking\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdelta_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mdelta_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mmasked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0mmasked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# get a copy that won't get overwritten by the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"immutable_outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cephfs/DSC261/project/Masker_Model/TextNTabularExplanations/src/joint_masker.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mask, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mmasked_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_mask_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tab_cols\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tab_cols\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# We unpack the string from the tuple and array and extend the masked_tab dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mmasked_tab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmasked_tab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4080\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4081\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4082\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4083\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4085\u001b[0m         elif (\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4141\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iset_not_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4160\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Columns must be same length as key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4162\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4163\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0migetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "config_type = 'pet_bert_alltext_shap' #parser.parse_args().config\n",
    "if \"baseline\" in config_type:\n",
    "    run_all_text_baseline_shap(merged_data, test_set_size=1000)\n",
    "\n",
    "else:\n",
    "    run_shap(merged_data, test_set_size=1000)\n",
    "gen_summary_shap_vals(merged_data, ctype = config_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ee0a95c-415d-4ed5-abdf-71579540dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shap.maskers import Masker\n",
    "from shap.maskers._tabular import _delta_masking\n",
    "from shap.maskers._text import (\n",
    "    SimpleTokenizer,\n",
    "    openers,\n",
    "    closers,\n",
    "    connectors,\n",
    "    TokenGroup,\n",
    ")\n",
    "import shap\n",
    "from shap.utils import safe_isinstance, MaskedModel, sample\n",
    "from shap.utils.transformers import (\n",
    "    parse_prefix_suffix_for_tokenizer,\n",
    "    SENTENCEPIECE_TOKENIZERS,\n",
    "    getattr_silent,\n",
    ")\n",
    "from shap.utils._exceptions import DimensionError\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy as sp\n",
    "import math\n",
    "\n",
    "\n",
    "class JointMasker(Masker):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tab_df,\n",
    "        text_cols,\n",
    "        cols_to_str_fn,\n",
    "        max_samples=100,\n",
    "        tokenizer=None,\n",
    "        mask_token=None,\n",
    "        collapse_mask_token=\"auto\",\n",
    "        output_type=\"string\",\n",
    "        tab_cluster_scale_factor=1,\n",
    "        tab_partition_tree=None,\n",
    "    ):\n",
    "        # Boiler plate from Text masker\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = SimpleTokenizer()\n",
    "        elif callable(tokenizer):\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            try:\n",
    "                self.tokenizer = SimpleTokenizer(tokenizer)\n",
    "            except:\n",
    "                raise Exception(  # pylint: disable=raise-missing-from\n",
    "                    \"The passed tokenizer cannot be wrapped as a masker because it does not have a __call__ \"\n",
    "                    + \"method, not can it be interpreted as a splitting regexp!\"\n",
    "                )\n",
    "\n",
    "        self.output_type = output_type\n",
    "        self.collapse_mask_token = collapse_mask_token\n",
    "        self.input_mask_token = mask_token\n",
    "        self.mask_token = mask_token  # could be recomputed later in this function\n",
    "        self.mask_token_id = mask_token if isinstance(mask_token, int) else None\n",
    "        # Optional scaling\n",
    "        self.tab_cluster_scale_factor = tab_cluster_scale_factor\n",
    "\n",
    "        # Tab\n",
    "        self.output_dataframe = False\n",
    "        if safe_isinstance(tab_df, \"pandas.core.frame.DataFrame\"):\n",
    "            self.tab_feature_names = list(tab_df.columns)\n",
    "            tab_df = tab_df.values\n",
    "            self.output_dataframe = True\n",
    "\n",
    "        # Tab clustering, partition tree calculated as in Tabular masker (correlation)\n",
    "        self.n_tab_cols = tab_df.shape[-1]\n",
    "        if tab_df.shape[-1] > 1:\n",
    "            # In order to cluster the tabular data, we replace null values with median\n",
    "            if tab_partition_tree is None:\n",
    "                self.tab_pt = sp.cluster.hierarchy.complete(\n",
    "                    sp.spatial.distance.pdist(\n",
    "                        pd.DataFrame(tab_df)\n",
    "                        .fillna(pd.DataFrame(tab_df).median())\n",
    "                        .values.T,\n",
    "                        metric=\"correlation\",\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.tab_pt = tab_partition_tree\n",
    "            # Necessary for adjusting text partition tree\n",
    "            self.n_tab_groups = len(self.tab_pt)\n",
    "        else:\n",
    "            # Needed to add this in to avoid errors when there is only one tabular column\n",
    "            self.tab_pt = None\n",
    "            self.n_tab_groups = 1\n",
    "\n",
    "        # Background dataset for tabular data set at max_samples\n",
    "        if hasattr(tab_df, \"shape\") and tab_df.shape[0] > max_samples:\n",
    "            tab_df = sample(tab_df, max_samples)\n",
    "\n",
    "        self.data = tab_df\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        self._masked_data = tab_df.copy()\n",
    "        self._last_mask = np.zeros(tab_df.shape[1], dtype=\"bool\")\n",
    "        self.tab_shape = tab_df.shape\n",
    "        self.supports_delta_masking = True\n",
    "\n",
    "        # Text\n",
    "        self.text_cols = text_cols\n",
    "        self.cols_to_text_fn = cols_to_str_fn\n",
    "        parsed_tokenizer_dict = parse_prefix_suffix_for_tokenizer(self.tokenizer)\n",
    "\n",
    "        self.keep_prefix = parsed_tokenizer_dict[\"keep_prefix\"]\n",
    "        self.keep_suffix = parsed_tokenizer_dict[\"keep_suffix\"]\n",
    "\n",
    "        self.text_data = True\n",
    "\n",
    "        if mask_token is None:\n",
    "            if getattr_silent(self.tokenizer, \"mask_token\") is not None:\n",
    "                self.mask_token = self.tokenizer.mask_token\n",
    "                self.mask_token_id = getattr_silent(self.tokenizer, \"mask_token_id\")\n",
    "                if self.collapse_mask_token == \"auto\":\n",
    "                    self.collapse_mask_token = False\n",
    "            else:\n",
    "                self.mask_token = \"...\"\n",
    "        else:\n",
    "            self.mask_token = mask_token\n",
    "\n",
    "        if self.mask_token_id is None:\n",
    "            self.mask_token_id = self.tokenizer(self.mask_token)[\"input_ids\"][\n",
    "                self.keep_prefix\n",
    "            ]\n",
    "\n",
    "        if self.collapse_mask_token == \"auto\":\n",
    "            self.collapse_mask_token = True\n",
    "\n",
    "        self.fixed_background = self.mask_token_id is None\n",
    "\n",
    "        self.default_batch_size = 5\n",
    "\n",
    "        # cache variables\n",
    "        self._s = None\n",
    "        self._tokenized_s_full = None\n",
    "        self._tokenized_s = None\n",
    "        self._segments_s = None\n",
    "\n",
    "        # flag that we return outputs that will not get changed by later masking calls\n",
    "        self.immutable_outputs = True\n",
    "\n",
    "    def __call__(self, mask, x):\n",
    "        \"\"\"\n",
    "        tab_mask_call returns a dataframe of shape (num_samples, num_tab_features)\n",
    "        text_mask_call returns a tuple of an array of a string, with the mask applied to the text\n",
    "        We join the text cols into a single string. This is independent of how they are joined in the model,\n",
    "        as we are only interested in the tokenization for the masking. This does mean that it won't work\n",
    "        for models that tokenize spaces (I think).\n",
    "\n",
    "        Possibly could do it instead by tokenizing all text features seperately, then joining them together\n",
    "        and handling the start and end tokens seperately.\n",
    "        \"\"\"\n",
    "        masked_tab = self.tab_mask_call(mask[: self.n_tab_cols], x[: self.n_tab_cols])\n",
    "        masked_text = self.text_mask_call(mask[self.n_tab_cols :], x[self.n_tab_cols :])\n",
    "\n",
    "        # We unpack the string from the tuple and array and extend the masked_tab dataframe\n",
    "        print(masked_tab, \"  Masked tab\")\n",
    "        masked_tab[self.text_cols] = masked_text[0]\n",
    "\n",
    "        return masked_tab.values\n",
    "\n",
    "    def tab_mask_call(self, mask, x):\n",
    "        \"\"\"\n",
    "        Taken from Tabular masker, with little change\n",
    "        \"\"\"\n",
    "        mask = self._standardize_mask(mask, x)\n",
    "\n",
    "        # make sure we are given a single sample\n",
    "        if len(x.shape) != 1 or x.shape[0] != self.data.shape[1]:\n",
    "            raise DimensionError(\n",
    "                \"The input passed for tabular masking does not match the background data shape!\"\n",
    "            )\n",
    "\n",
    "        # if mask is an array of integers then we are doing delta masking\n",
    "        if np.issubdtype(mask.dtype, np.integer):\n",
    "            variants = ~self.invariants(x)\n",
    "            curr_delta_inds = np.zeros(len(mask), dtype=np.int)\n",
    "            num_masks = (mask >= 0).sum()\n",
    "            varying_rows_out = np.zeros((num_masks, self.tab_shape[0]), dtype=\"bool\")\n",
    "            masked_inputs_out = np.zeros(\n",
    "                (num_masks * self.tab_shape[0], self.tab_shape[1])\n",
    "            )\n",
    "            self._last_mask[:] = False\n",
    "            self._masked_data[:] = self.data\n",
    "            _delta_masking(\n",
    "                mask,\n",
    "                x,\n",
    "                curr_delta_inds,\n",
    "                varying_rows_out,\n",
    "                self._masked_data,\n",
    "                self._last_mask,\n",
    "                self.data,\n",
    "                variants,\n",
    "                masked_inputs_out,\n",
    "                MaskedModel.delta_mask_noop_value,\n",
    "            )\n",
    "            if self.output_dataframe:\n",
    "                return (\n",
    "                    pd.DataFrame(masked_inputs_out, columns=self.tab_feature_names),\n",
    "                ), varying_rows_out\n",
    "\n",
    "            return (masked_inputs_out,), varying_rows_out\n",
    "\n",
    "        # otherwise we update the whole set of masked data for a single sample\n",
    "        self._masked_data[:] = x * mask + self.data * np.invert(mask)\n",
    "        self._last_mask[:] = mask\n",
    "\n",
    "        if self.output_dataframe:\n",
    "            return pd.DataFrame(self._masked_data, columns=self.tab_feature_names)\n",
    "\n",
    "        return (self._masked_data,)\n",
    "\n",
    "    def text_mask_call(self, mask, s):\n",
    "        \"\"\"\n",
    "        Taken from text masker, changed to ensure that the mask tokens are not collapsed across\n",
    "        different text features\n",
    "        \"\"\"\n",
    "        mask = self._standardize_mask(mask, s)\n",
    "        self._update_s_cache(s)\n",
    "\n",
    "        # if we have a fixed prefix or suffix then we need to grow the mask to account for that\n",
    "        if self.keep_prefix > 0 or self.keep_suffix > 0:\n",
    "            mask = mask.copy()\n",
    "            mask[: self.keep_prefix] = True\n",
    "            mask[-self.keep_suffix :] = True\n",
    "        #  split mask into groups based on [len(col) for col in self._tokenized_s] ([9, 11, 12])\n",
    "        mask_per_col = np.split(\n",
    "            mask, np.cumsum([len(col) for col in self._tokenized_s])[:-1]\n",
    "        )\n",
    "        out = []\n",
    "        for col_idx, mask in enumerate(mask_per_col):\n",
    "            if self.output_type == \"string\":\n",
    "                out_parts = []\n",
    "                col_out = []\n",
    "                is_previous_appended_token_mask_token = False\n",
    "                sep_token = getattr_silent(self.tokenizer, \"sep_token\")\n",
    "                for i, v in enumerate(mask):\n",
    "                    # mask ignores separator tokens and keeps them unmasked\n",
    "                    if v or sep_token == self._segments_s[col_idx][i]:\n",
    "                        out_parts.append(self._segments_s[col_idx][i])\n",
    "                        is_previous_appended_token_mask_token = False\n",
    "                        # Change in here to show diffs between desc and title\n",
    "                    else:\n",
    "                        # If we don't collapse any mask tokens then we add another mask token\n",
    "                        # Or if the previous appended token was not a mask token\n",
    "                        if not self.collapse_mask_token or (\n",
    "                            self.collapse_mask_token\n",
    "                            and not is_previous_appended_token_mask_token\n",
    "                        ):\n",
    "                            out_parts.append(\" \" + self.mask_token)\n",
    "                            is_previous_appended_token_mask_token = True\n",
    "                            # Length 9, 0,1,2,3,4 is the first group, 5,6 2nd and 7,8 3rd\n",
    "                            # All the masks are false, so\n",
    "                    # if i in self._sent_split_idxs:\n",
    "                    #     out.append(\"\".join(out_parts))\n",
    "                    #     out_parts = []\n",
    "                    #     is_previous_appended_token_mask_token = False\n",
    "                col_out.append(\"\".join(out_parts))\n",
    "                # out.append(\"\".join(\"\".join(out_parts).split(self.tokenizer.sep_token)))\n",
    "\n",
    "                for i in range(len(col_out)):\n",
    "                    col_out[i] = re.sub(r\"[\\s]+\", \" \", col_out[i]).strip()\n",
    "                    if safe_isinstance(self.tokenizer, SENTENCEPIECE_TOKENIZERS):\n",
    "                        col_out[i] = col_out[i].replace(\"▁\", \" \")\n",
    "\n",
    "            else:\n",
    "                if self.mask_token_id is None:\n",
    "                    col_out = self._tokenized_s[col_idx][mask]\n",
    "                else:\n",
    "                    col_out = np.array(\n",
    "                        [\n",
    "                            self._tokenized_s[col_idx][i]\n",
    "                            if mask[i]\n",
    "                            else self.mask_token_id\n",
    "                            for i in range(len(mask))\n",
    "                        ]\n",
    "                    )\n",
    "            out.extend(col_out)\n",
    "        # for some sentences with strange configurations around the separator tokens, tokenizer encoding/decoding may contain\n",
    "        # extra unnecessary tokens, for example ''. you may want to strip out spaces adjacent to separator tokens. Refer to PR\n",
    "        # for more details.\n",
    "        # print(out)\n",
    "        assert len(out) == len(self._tokenized_s)\n",
    "        # return\n",
    "        return (np.array(out),)\n",
    "\n",
    "    def _update_s_cache(self, s):\n",
    "        # \"\"\"Same as Text masker\"\"\"\n",
    "        joined_s = \"\".join(col for col in s)\n",
    "        if self._s != joined_s:\n",
    "            self._s = joined_s\n",
    "            if len(s) == 1:\n",
    "                tokens, token_ids = self.token_segments(s[0])\n",
    "                self._tokenized_s = [np.array(token_ids)]\n",
    "                self._segments_s = [np.array(tokens)]\n",
    "            else:\n",
    "                all_tokens = []\n",
    "                all_token_ids = []\n",
    "                for col_idx, text_col in enumerate(s):\n",
    "                    col_tokens, col_token_ids = self.token_segments(text_col)\n",
    "                    \"\"\"\n",
    "                    We differentiate these because the first column we need to get \n",
    "                    rid of the end of sentence token, the last column we need to get \n",
    "                    rid of the start of sentence token and the middle columns we need\n",
    "                    to get rid of both.\n",
    "                    \n",
    "                    We also need to make allowances for when the tokenizer doesn't have\n",
    "                    a start or end of sentence token.\n",
    "                    \"\"\"\n",
    "                    # First col\n",
    "                    if col_idx == 0:\n",
    "                        col_token_ids = (\n",
    "                            col_token_ids[:-1] if self.keep_suffix else col_token_ids\n",
    "                        )\n",
    "                        col_tokens = col_tokens[:-1] if self.keep_suffix else col_tokens\n",
    "                    # Middle cols\n",
    "                    elif col_idx != len(s) - 1:\n",
    "                        col_token_ids = (\n",
    "                            col_token_ids[self.keep_prefix : -1]\n",
    "                            if self.keep_suffix\n",
    "                            else col_token_ids[self.keep_prefix :]\n",
    "                        )\n",
    "                        col_tokens = (\n",
    "                            col_tokens[self.keep_prefix : -1]\n",
    "                            if self.keep_suffix\n",
    "                            else col_tokens[self.keep_prefix :]\n",
    "                        )\n",
    "                    # Last col\n",
    "                    else:\n",
    "                        col_token_ids = col_token_ids[self.keep_prefix :]\n",
    "                        col_tokens = col_tokens[self.keep_prefix :]\n",
    "                    all_tokens.append(np.array(col_tokens))\n",
    "                    all_token_ids.append(np.array(col_token_ids))\n",
    "                self._tokenized_s = all_token_ids\n",
    "                self._segments_s = all_tokens\n",
    "\n",
    "    def token_segments(self, s):\n",
    "        \"\"\"Same as Text masker\"\"\"\n",
    "        \"\"\" Returns the substrings associated with each token in the given string.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            token_data = self.tokenizer(s, return_offsets_mapping=True)\n",
    "            offsets = token_data[\"offset_mapping\"]\n",
    "            offsets = [(0, 0) if o is None else o for o in offsets]\n",
    "            parts = [\n",
    "                s[offsets[i][0] : max(offsets[i][1], offsets[i + 1][0])]\n",
    "                for i in range(len(offsets) - 1)\n",
    "            ]\n",
    "            parts.append(s[offsets[len(offsets) - 1][0] : offsets[len(offsets) - 1][1]])\n",
    "            return parts, token_data[\"input_ids\"]\n",
    "        except (\n",
    "            NotImplementedError,\n",
    "            TypeError,\n",
    "        ):  # catch lack of support for return_offsets_mapping\n",
    "            token_ids = self.tokenizer(s)[\"input_ids\"]\n",
    "            if hasattr(self.tokenizer, \"convert_ids_to_tokens\"):\n",
    "                tokens = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            else:\n",
    "                tokens = [self.tokenizer.decode([id]) for id in token_ids]\n",
    "            if hasattr(self.tokenizer, \"get_special_tokens_mask\"):\n",
    "                special_tokens_mask = self.tokenizer.get_special_tokens_mask(\n",
    "                    token_ids, already_has_special_tokens=True\n",
    "                )\n",
    "                # avoid masking separator tokens, but still mask beginning of sentence and end of sentence tokens\n",
    "                special_keep = [\n",
    "                    getattr_silent(self.tokenizer, \"sep_token\"),\n",
    "                    getattr_silent(self.tokenizer, \"mask_token\"),\n",
    "                ]\n",
    "                for i, v in enumerate(special_tokens_mask):\n",
    "                    if v == 1 and (\n",
    "                        tokens[i] not in special_keep\n",
    "                        or i + 1 == len(special_tokens_mask)\n",
    "                    ):\n",
    "                        tokens[i] = \"\"\n",
    "\n",
    "            # add spaces to separate the tokens (since we want segments not tokens)\n",
    "            if safe_isinstance(self.tokenizer, SENTENCEPIECE_TOKENIZERS):\n",
    "                for i, v in enumerate(tokens):\n",
    "                    if v.startswith(\"_\"):\n",
    "                        tokens[i] = \" \" + tokens[i][1:]\n",
    "            else:\n",
    "                for i, v in enumerate(tokens):\n",
    "                    if v.startswith(\"##\"):\n",
    "                        tokens[i] = tokens[i][2:]\n",
    "                    elif v != \"\" and i != 0:\n",
    "                        tokens[i] = \" \" + tokens[i]\n",
    "\n",
    "            return tokens, token_ids\n",
    "\n",
    "    def clustering(self, s):\n",
    "        \"\"\"\n",
    "        [Dendograms explained:]\n",
    "\n",
    "        Dendrograms creation works by having each one of the base leaves as a number, then\n",
    "        labelling each one of the new created nodes a number following the last leaf number.\n",
    "        Columns 0 and 1 are the two nodes that are being joined, column 2 is the similarity\n",
    "        between the two nodes and column 3 is the number of leaves in the new node.\n",
    "\n",
    "        eg for array(\n",
    "            [[0. , 1. , 0.4, 2. ],\n",
    "            [2. , 3. , 0.4, 2. ],\n",
    "            [6. , 4. , 0.6, 3. ],\n",
    "            [5. , 7. , 1. , 5. ]]\n",
    "        )\n",
    "\n",
    "        In this case we know can see from the 4th row that there are 5 leaves in the final node:\n",
    "        [0,1,2,3,4]. Each grouping (row) is also given a number starting from one more than the\n",
    "        last leaf node. Therefore the pairing of (0,1) from row 0 is labelled as 5, (2,3) from\n",
    "        row 1 is labelled as 6, (6,4) from row 2 is labelled as 7 and (5,7) from row 3 is labelled\n",
    "        as 8. This tells the algorithm how to plot the dendrogram. For example row 3 tells us that\n",
    "        the group of (2,3) ie node 6 is joined to node 4, which is the group of (6,4) ie node 7.\n",
    "\n",
    "        With this knowledge we can adjust the text dendrogram in order to join the tabular dendrogram\n",
    "        on from the left side.\n",
    "\n",
    "        If we initially have:\n",
    "            A tabular dendogram which has:\n",
    "                * n leaves (labelled [0,n-1]\n",
    "                * m groups (labelled [n, n+m-1])\n",
    "            A text dendrogram which has:\n",
    "                * k leaves (labelled [0,k-1]\n",
    "                * l groups (labelled [k, k+l-1])\n",
    "\n",
    "        This will now become a joint dendogram which has :\n",
    "        * n tabular leaves (labelled [0,n-1]\n",
    "        * k text leaves (labelled [n, n+k-1])\n",
    "        * m tabular groups (labelled [n+k, n+k+m-1])\n",
    "        * l text groups (labelled [n+k+m, n+k+m+l-1])\n",
    "\n",
    "        \"\"\"\n",
    "        # text = \" \".join(str(s) for s in x[self.n_tab_cols :])\n",
    "        # text = self.tokenizer.sep_token.join(str(s) for s in s[self.n_tab_cols :])\n",
    "        text = s[self.n_tab_cols :]\n",
    "        # joiner = getattr_silent(self.tokenizer, \"sep_token\")\n",
    "        # joiner = \" \" if joiner is None else joiner\n",
    "        # text = joiner.join(str(s) for s in s[self.n_tab_cols :])\n",
    "\n",
    "        # Same as Text masker\n",
    "        ################################\n",
    "        self._update_s_cache(text)\n",
    "        special_tokens = []\n",
    "        sep_token = getattr_silent(self.tokenizer, \"sep_token\")\n",
    "        if sep_token is None:\n",
    "            special_tokens = []\n",
    "        else:\n",
    "            special_tokens = [sep_token]\n",
    "\n",
    "        text_pts = []\n",
    "        all_tokens = []\n",
    "        for col in range(len(self._tokenized_s)):\n",
    "            # convert the text segments to tokens that the partition tree function expects\n",
    "            tokens = []\n",
    "            space_end = re.compile(r\"^.*\\W$\")\n",
    "            letter_start = re.compile(r\"^[A-z]\")\n",
    "            for i, v in enumerate(self._segments_s[col]):\n",
    "                if (\n",
    "                    i > 0\n",
    "                    and space_end.match(self._segments_s[col][i - 1]) is None\n",
    "                    and letter_start.match(v) is not None\n",
    "                    and tokens[i - 1] != \"\"\n",
    "                ):\n",
    "                    tokens.append(\"##\" + v.strip())\n",
    "                else:\n",
    "                    tokens.append(v.strip())\n",
    "\n",
    "            # text_pt = partition_tree(tokens, special_tokens, self.sent_indices)\n",
    "            text_pt = shap.maskers._text.partition_tree(tokens, special_tokens)\n",
    "            if len(text_pt) == 0:\n",
    "                text_pt = np.array([[0, np.inf, 0, 1]])\n",
    "                # text_pt[:, 2] = text_pt[:, 3]\n",
    "                # text_pt[:, 2] /= text_pt[:, 2].max()\n",
    "            text_pts.append(text_pt)\n",
    "            all_tokens.extend(tokens)\n",
    "        ################################\n",
    "\n",
    "        if len(text_pts) == 1:\n",
    "            text_pt = text_pts[0]\n",
    "        else:\n",
    "            text_pt = join_dendograms(text_pts)\n",
    "        n_text_leaves = len(all_tokens)\n",
    "        n_text_groups = len(text_pt)\n",
    "\n",
    "        if self.tab_pt is not None:\n",
    "            # References to non-leaf nodes need to be shifted by the number of new leaves\n",
    "            Z_join = np.zeros([self.n_tab_groups + n_text_groups + 1, 4])\n",
    "            # Put tab first, then text\n",
    "            Z_join[: self.n_tab_groups, :2] = np.where(\n",
    "                self.tab_pt[:, :2] >= self.n_tab_cols,\n",
    "                self.tab_pt[:, :2] + n_text_leaves,\n",
    "                self.tab_pt[:, :2],\n",
    "            )\n",
    "            Z_join[self.n_tab_groups : -1, :2] = np.where(\n",
    "                text_pt[:, :2] >= n_text_leaves,\n",
    "                text_pt[:, :2] + self.n_tab_cols + self.n_tab_groups,\n",
    "                text_pt[:, :2] + self.n_tab_cols,\n",
    "            )\n",
    "\n",
    "            # Scale tab_pt\n",
    "            self.tab_pt[:, 2] /= self.tab_pt[:, 2].max() * self.tab_cluster_scale_factor\n",
    "\n",
    "            # 3rd and 4th columns are left unchanged\n",
    "            Z_join[: self.n_tab_groups, 2:] = self.tab_pt[:, 2:]\n",
    "            Z_join[self.n_tab_groups : -1, 2:] = text_pt[:, 2:]\n",
    "\n",
    "            # Create top join, joining the text and tab dendrograms together\n",
    "            top_tab_node = self.n_tab_cols + n_text_leaves + self.n_tab_groups - 1\n",
    "            top_text_node = top_tab_node + n_text_groups\n",
    "            # Set similarity of top node to 1.2\n",
    "            Z_join[-1, :] = np.array(\n",
    "                [top_tab_node, top_text_node, 1.2, n_text_leaves + self.n_tab_cols]\n",
    "            )\n",
    "        else:\n",
    "            # If there is no tab_pt then this means there is only one tab column. In the resulting dendogram,\n",
    "            # the single tab column will be joined to the text dendrogram on the left\n",
    "            Z_join = np.zeros([n_text_groups + self.n_tab_cols, 4])\n",
    "            Z_join[:-1, :2] = np.where(\n",
    "                text_pt[:, :2] >= n_text_leaves,\n",
    "                text_pt[:, :2] + self.n_tab_cols,\n",
    "                text_pt[:, :2] + self.n_tab_cols,\n",
    "            )\n",
    "            # 3rd and 4th columns are left unchanged\n",
    "            Z_join[:-1, 2:] = text_pt[:, 2:]\n",
    "\n",
    "            top_text_node = n_text_leaves + n_text_groups\n",
    "            Z_join[-1, :] = np.array(\n",
    "                [0, top_text_node, 1.2, n_text_leaves + self.n_tab_cols]\n",
    "            )\n",
    "        return Z_join\n",
    "\n",
    "    def shape(self, s):\n",
    "        \"\"\"The shape of what we return as a masker.\"\"\"\n",
    "        self._update_s_cache(s[self.n_tab_cols :])\n",
    "        return (\n",
    "            self.max_samples,\n",
    "            self.n_tab_cols + sum([len(col) for col in self._tokenized_s]),\n",
    "        )\n",
    "\n",
    "    def mask_shapes(self, s):\n",
    "        \"\"\"The shape of the masks we expect.\"\"\"\n",
    "        self._update_s_cache(s[self.n_tab_cols :])\n",
    "        return [(self.n_tab_cols + sum([len(col) for col in self._tokenized_s]),)]\n",
    "\n",
    "    def feature_names(self, s):\n",
    "        \"\"\"The names of the features for each mask position for the given input string.\"\"\"\n",
    "        self._update_s_cache(s[self.n_tab_cols :])\n",
    "        return [self.tab_feature_names + [v for col in self._segments_s for v in col]]\n",
    "\n",
    "\n",
    "class Token:\n",
    "    \"\"\"A token representation used for token clustering.\n",
    "    Same as Text masker but sent_no added to track which sentence the token is in\"\"\"\n",
    "\n",
    "    def __init__(self, value, sent_no):\n",
    "        self.s = value\n",
    "        self.sent_no = sent_no  # added sent_no to track which sentence the token is in\n",
    "        if value in openers or value in closers:\n",
    "            self.balanced = False\n",
    "        else:\n",
    "            self.balanced = True\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.s\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self.balanced:\n",
    "            return self.s + \"!\"\n",
    "        return self.s\n",
    "\n",
    "\n",
    "def merge_score(group1, group2, special_tokens):\n",
    "    \"\"\"Compute the score of merging two token groups.\n",
    "\n",
    "    special_tokens: tokens (such as separator tokens) that should be grouped last\n",
    "    \"\"\"\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # Changed\n",
    "    ########################################\n",
    "    if group1[-1].sent_no != group2[0].sent_no:\n",
    "        score -= (\n",
    "            math.inf\n",
    "        )  # subtracting infinity to create lowest score and ensure combining these groups last\n",
    "\n",
    "    ########################################\n",
    "\n",
    "    # ensures special tokens are combined last, so 1st subtree is 1st sentence and 2nd subtree is 2nd sentence\n",
    "    if len(special_tokens) > 0:\n",
    "        if group1[-1].s in special_tokens and group2[0].s in special_tokens:\n",
    "            score -= (\n",
    "                math.inf\n",
    "            )  # subtracting infinity to create lowest score and ensure combining these groups last\n",
    "\n",
    "    # merge broken-up parts of words first\n",
    "    if group2[0].s.startswith(\"##\"):\n",
    "        score += 20\n",
    "\n",
    "    # merge apostrophe endings next\n",
    "    if group2[0].s == \"'\" and (\n",
    "        len(group2) == 1 or (len(group2) == 2 and group2[1].s in [\"t\", \"s\"])\n",
    "    ):\n",
    "        score += 15\n",
    "    if group1[-1].s == \"'\" and group2[0].s in [\"t\", \"s\"]:\n",
    "        score += 15\n",
    "\n",
    "    start_ctrl = group1[0].s.startswith(\"[\") and group1[0].s.endswith(\"]\")\n",
    "    end_ctrl = group2[-1].s.startswith(\"[\") and group2[-1].s.endswith(\"]\")\n",
    "\n",
    "    if (start_ctrl and not end_ctrl) or (end_ctrl and not start_ctrl):\n",
    "        score -= 1000\n",
    "    if group2[0].s in openers and not group2[0].balanced:\n",
    "        score -= 100\n",
    "    if group1[-1].s in closers and not group1[-1].balanced:\n",
    "        score -= 100\n",
    "\n",
    "    # attach surrounding an openers and closers a bit later\n",
    "    if group1[0].s in openers and not group2[-1] in closers:\n",
    "        score -= 2\n",
    "\n",
    "    # reach across connectors later\n",
    "    if group1[-1].s in connectors or group2[0].s in connectors:\n",
    "        score -= 2\n",
    "\n",
    "    # reach across commas later\n",
    "    if group1[-1].s == \",\":\n",
    "        score -= 10\n",
    "    if group2[0].s == \",\":\n",
    "        if len(group2) > 1:  # reach across\n",
    "            score -= 10\n",
    "        else:\n",
    "            score -= 1\n",
    "\n",
    "    # reach across sentence endings later\n",
    "    if group1[-1].s in [\".\", \"?\", \"!\"]:\n",
    "        score -= 20\n",
    "    if group2[0].s in [\".\", \"?\", \"!\"]:\n",
    "        if len(group2) > 1:  # reach across\n",
    "            score -= 20\n",
    "        else:\n",
    "            score -= 1\n",
    "\n",
    "    score -= len(group1) + len(group2)\n",
    "    # print(group1, group2, score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def join_dendograms(pts):\n",
    "    n_leaves = [int(max(pt[:, 3])) for pt in pts]\n",
    "    n_empty = sum([1 for n in n_leaves if n == 1])\n",
    "    n_groups = [len(pt) for pt in pts]\n",
    "\n",
    "    # Needed to add this because could join two together but you'd still need\n",
    "    # an extra row to join the next one on. So if there are two single words\n",
    "    # then we need to reduce n_empty by 1\n",
    "    n_empty -= n_leaves[0] == 1 and n_leaves[1] == 1\n",
    "\n",
    "    pt_join = np.zeros((sum(n_groups) + len(pts) - n_empty - 1, 4))\n",
    "    # For the first partition tree the leaves (ie the words/features) are unchanged,\n",
    "    # but the group numbers need to be shifted by the total number of leaves, less\n",
    "    # the number of leaves in the first tree which are already accounted for\n",
    "    pt_join[: n_groups[0], :2] = np.where(\n",
    "        pts[0][:, :2] < n_leaves[0],\n",
    "        pts[0][:, :2],\n",
    "        pts[0][:, :2] + sum(n_leaves) - n_leaves[0],\n",
    "    )\n",
    "    # For the remaining trees the leaves need to be shifted by the sum of the leaves\n",
    "    # of the previous trees. The group numbers need to be shifted by the total number\n",
    "    # of leaves, less the number of leaves in the current tree which are already accounted for,\n",
    "    # plus the number of groups in the previous trees\n",
    "    g_cs = np.cumsum(n_groups)\n",
    "    l_cs = np.cumsum(n_leaves)\n",
    "    for pt, i in zip(pts[1:], range(1, len(pts))):\n",
    "        if np.equal(pt, np.array([[0, np.inf, 0, 1]])).all():  # empty tree\n",
    "            pt_join[g_cs[i - 1] : g_cs[i], :2] = np.array([l_cs[i - 1], np.inf])\n",
    "        else:\n",
    "            pt_join[g_cs[i - 1] : g_cs[i], :2] = np.where(\n",
    "                pt[:, :2] < n_leaves[i],\n",
    "                pt[:, :2] + l_cs[i - 1],\n",
    "                pt[:, :2] + sum(n_leaves) - n_leaves[i] + g_cs[i - 1],\n",
    "            )\n",
    "\n",
    "    # 3rd and 4th columns are left unchanged\n",
    "    pt_join[: -(len(pts) - n_empty - 1), 2:] = np.concatenate([pt[:, 2:] for pt in pts])\n",
    "\n",
    "    # If a text feature only has one word then we need to do some extra work\n",
    "    top_nodes0 = [i + sum(n_leaves) - 1 for i in g_cs]\n",
    "    n_in_top_nodes0 = list(pt_join[[i + -1 for i in g_cs], -1])\n",
    "    top_node_acntd_for = [False for _ in g_cs]\n",
    "\n",
    "    # If a text feature only has one word then we join it to the top node of the\n",
    "    # previous tree and increment the number of words in that top node\n",
    "    for i in range(1, len(n_leaves)):\n",
    "        if n_leaves[i] == 1:\n",
    "            pt_join[g_cs[i] - 1, 1] = top_nodes0[i - 1]\n",
    "            pt_join[g_cs[i] - 1, 2] = 1\n",
    "            pt_join[g_cs[i] - 1, 3] = n_in_top_nodes0[i - 1] + 1\n",
    "            top_node_acntd_for[i - 1] = True\n",
    "            n_in_top_nodes0[i] += n_in_top_nodes0[i - 1]\n",
    "\n",
    "    # Now some of the top nodes will have been accounted for, so we need to remove them\n",
    "    top_nodes = [i for idx, i in enumerate(top_nodes0) if not top_node_acntd_for[idx]]\n",
    "    n_in_top_nodes = [\n",
    "        i for idx, i in enumerate(n_in_top_nodes0) if not top_node_acntd_for[idx]\n",
    "    ]\n",
    "\n",
    "    #########################################################################################\n",
    "    # This bit is all to take into account the case where the first group is a single token\n",
    "    # This is a fringe case which will only happen if the first group is a single token and\n",
    "    # the tokenizer does not have a start of sentence token\n",
    "\n",
    "    if pt_join[0, 1] == np.inf:\n",
    "        # We have to check that the second group is not a single word too\n",
    "        if n_leaves[1] == 1:\n",
    "            # first and second groups just form a single group\n",
    "            pt_join[0, 1] = 1\n",
    "            pt_join[0, 3] = 2\n",
    "            # pop the second group\n",
    "            pt_join = np.delete(pt_join, 1, 0)\n",
    "            # adjust group references: if there is a number > sum(n_leaves) then minus 1 from it\n",
    "            pt_join[:, :2] = np.where(\n",
    "                pt_join[:, :2] > sum(n_leaves), pt_join[:, :2] - 1, pt_join[:, :2]\n",
    "            )\n",
    "            # adjust top_nodes: if there is a number > sum(n_leaves) then minus 1 from it\n",
    "            top_nodes = [i - 1 for i in top_nodes]\n",
    "            # adjust g_cs because we use it later\n",
    "            g_cs = [i - 1 for i in g_cs[1:]]\n",
    "        else:\n",
    "            # Connect it to the top node of the next tree\n",
    "            pt_join[0, 1] = top_nodes[1]\n",
    "            pt_join[0, 2] = 1\n",
    "            pt_join[0, 3] = n_in_top_nodes[1] + 1\n",
    "            # rearrange groups such that the first row is now behind the seconrd group\n",
    "            second_grp_size = top_nodes[1] - top_nodes[0]\n",
    "            new_order = (\n",
    "                list(range(1, second_grp_size + 1))\n",
    "                + [0]\n",
    "                + list(range(1 + second_grp_size, pt_join.shape[0]))\n",
    "            )\n",
    "            pt_join = pt_join[new_order, :]\n",
    "            # adjust group references: if there is a number >= sum(n_leaves) and <= top_nodes[1]\n",
    "            # then minus 1 from it\n",
    "            pt_join[:, :2] = np.where(\n",
    "                (pt_join[:, :2] >= sum(n_leaves)) & (pt_join[:, :2] <= top_nodes[1]),\n",
    "                pt_join[:, :2] - 1,\n",
    "                pt_join[:, :2],\n",
    "            )\n",
    "            # redefine top_nodes and n_in_top_nodes, swapping the first two elements\n",
    "            top_nodes = top_nodes[1:]\n",
    "            n_in_top_nodes = n_in_top_nodes[1:]\n",
    "            n_in_top_nodes[0] += 1\n",
    "    #########################################################################################\n",
    "\n",
    "    # Now we need to join the top nodes together\n",
    "    joiner_rows = []\n",
    "    while len(top_nodes) > 1:\n",
    "        first_two = sum(n_in_top_nodes[:2])\n",
    "        joiner_row = top_nodes[:2] + [0, first_two]\n",
    "        joiner_rows.append(joiner_row)\n",
    "        top_nodes = top_nodes[2:]\n",
    "        n_in_top_nodes = n_in_top_nodes[2:]\n",
    "        if len(top_nodes) > 0:\n",
    "            top_nodes.insert(0, top_nodes[-1] + len(joiner_rows))\n",
    "            n_in_top_nodes.insert(0, first_two)\n",
    "    pt_join[g_cs[-1] :] = np.array(joiner_rows)\n",
    "    pt_join[:, 2] = pt_join[:, 3]\n",
    "    pt_join[:, 2] /= pt_join[:, 2].max()\n",
    "    return pt_join\n",
    "\n",
    "\n",
    "def partition_tree(decoded_tokens, special_tokens, sent_indices):\n",
    "    \"\"\"Build a heriarchial clustering of tokens that align with sentence structure.\n",
    "\n",
    "    Note that this is fast and heuristic right now.\n",
    "    TODO: Build this using a real constituency parser.\n",
    "    \"\"\"\n",
    "    # Only difference is that we add sent_indices to the Token class\n",
    "    #############################\n",
    "    token_groups = [\n",
    "        TokenGroup([Token(t, idx)], i)\n",
    "        for i, (t, idx) in enumerate(zip(decoded_tokens, sent_indices))\n",
    "    ]\n",
    "    #############################\n",
    "    #     print(token_groups)\n",
    "    M = len(decoded_tokens)\n",
    "    new_index = M\n",
    "    clustm = np.zeros((M - 1, 4))\n",
    "    for i in range(len(token_groups) - 1):\n",
    "        scores = [\n",
    "            merge_score(token_groups[i], token_groups[i + 1], special_tokens)\n",
    "            for i in range(len(token_groups) - 1)\n",
    "        ]\n",
    "\n",
    "        #         print(scores)\n",
    "        ind = np.argmax(scores)\n",
    "\n",
    "        lind = token_groups[ind].index\n",
    "        rind = token_groups[ind + 1].index\n",
    "        clustm[new_index - M, 0] = token_groups[ind].index\n",
    "        clustm[new_index - M, 1] = token_groups[ind + 1].index\n",
    "        clustm[new_index - M, 2] = -scores[ind]\n",
    "        clustm[new_index - M, 3] = (clustm[lind - M, 3] if lind >= M else 1) + (\n",
    "            clustm[rind - M, 3] if rind >= M else 1\n",
    "        )\n",
    "\n",
    "        token_groups[ind] = token_groups[ind] + token_groups[ind + 1]\n",
    "        token_groups[ind].index = new_index\n",
    "\n",
    "        # track balancing of openers/closers\n",
    "        if (\n",
    "            token_groups[ind][0].s in openers\n",
    "            and token_groups[ind + 1][-1].s == openers[token_groups[ind][0].s]\n",
    "        ):\n",
    "            token_groups[ind][0].balanced = True\n",
    "            token_groups[ind + 1][-1].balanced = True\n",
    "\n",
    "        token_groups.pop(ind + 1)\n",
    "        new_index += 1\n",
    "\n",
    "    # negative means we should never split a group, so we add 10 to ensure these are very tight groups\n",
    "    # (such as parts of the same word)\n",
    "    clustm[:, 2] = clustm[:, 2] + 10\n",
    "\n",
    "    return clustm\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run_shap_vals('tab')\n",
    "    # run_shap_vals(\"joint\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01e2e0-dcf9-4b74-9d81-c0f989fc30ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba8fb2-e70e-4e9e-ba12-ddd913dc12a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
