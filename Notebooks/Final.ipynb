{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4a7115-0108-407b-b55c-d3c9a8e03d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/cephfs/DSC261/project/Masker_Model/TextNTabularExplanations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777a2a05-3e17-44ad-be96-1063f8a2bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install TensorFlow==2.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fb9638-ce7d-4d59-977e-6b6ac926510a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtensorflow\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eca6bed-c02a-4f13-add6-7ccabd1eb11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 19:10:54.759036: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 19:10:54.759112: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 19:10:54.759149: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 19:10:54.768327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 19:10:55.634864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from src.utils import legacy_get_dataset_info\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.utils import token_segments, text_ft_index_ends, format_text_pred, ConfigLoader\n",
    "\n",
    "# from src.models import Model\n",
    "import lightgbm as lgb\n",
    "from src.models import AllAsTextModel\n",
    "from src.joint_masker import JointMasker\n",
    "import argparse\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb12b85-192a-443e-b2c3-0b082412fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#     \"--ds_type\",\n",
    "#     type=str,\n",
    "#     default=\"airbnb\",\n",
    "#     help=\"Name of dataset to use\",\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--text_model_code\",\n",
    "#     type=str,\n",
    "#     default=\"disbert\",\n",
    "#     help=\"Code name for text model to use\",\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--repeat_idx\",\n",
    "#     type=int,\n",
    "#     default=None,\n",
    "#     help=\"For the explainability consistency experiment, which repeat to use\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ce9847-ee14-4adf-9c04-14c1b332ec6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_shap\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m,\n\u001b[1;32m      3\u001b[0m     max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m     test_set_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      5\u001b[0m ):\n\u001b[1;32m      7\u001b[0m     ds_name \u001b[38;5;241m=\u001b[39m merged_df\n\u001b[1;32m      8\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m      9\u001b[0m         ds_name,\n\u001b[1;32m     10\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# download_mode=\"force_redownload\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "def run_shap(\n",
    "    merged_df = merged_df,\n",
    "    max_samples=100,\n",
    "    test_set_size=100,\n",
    "):\n",
    "\n",
    "    ds_name = merged_df\n",
    "    train_df = load_dataset(\n",
    "        ds_name,\n",
    "        split=\"train\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        ds_name,\n",
    "        split=\"test\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    train_df, test_df = train_test_split(merged_data.drop('AdoptionSpeed', axis = 1), random_state = 42, test_size = 0.2)\n",
    "    y_train = train_df['AdoptionSpeed']\n",
    "    test_df = test_df.sample(test_set_size, random_state=11)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, max_length = 256,\n",
    "                                             padding = 'max_length', truncation = True, \n",
    "                                              return_token_type_ids= False, return_tensors = 'tf')\n",
    "\n",
    "    model1 = load_model(\"/cephfs/DSC261/project/model/best_bert_unimodal.hdf5\", custom_objects={'f1_metric': f1_metric, 'TFBertModel': TFBertModel})\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased')  \n",
    "    model1.config = config\n",
    "\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=args.my_text_model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=\"cuda:0\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        top_k=None,\n",
    "    )\n",
    "    # Define how to convert all columns to a single string\n",
    "\n",
    "    cols_to_str_fn = lambda array: \" | \".join(\n",
    "        [f\"{col}: {val}\" for col, val in zip(di.tab_cols + di.text_cols, array)]\n",
    "    )\n",
    "    \n",
    "\n",
    "    model = AllAsTextModel(\n",
    "        text_pipeline=text_pipeline,\n",
    "        cols_to_str_fn=cols_to_str_fn,\n",
    "    )\n",
    "\n",
    "    np.random.seed(1)\n",
    "    x = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    # We need to load the ordinal dataset so that we can calculate the correlations for the masker\n",
    "    ord_train_df = load_dataset(di.ord_ds_name, split=\"train\").to_pandas()\n",
    "\n",
    "    # Clustering only valid if there is more than one column\n",
    "    if len(di.tab_cols) > 1:\n",
    "        tab_pt = sp.cluster.hierarchy.complete(\n",
    "            sp.spatial.distance.pdist(\n",
    "                ord_train_df[di.tab_cols]\n",
    "                .fillna(ord_train_df[di.tab_cols].median())\n",
    "                .values.T,\n",
    "                metric=\"correlation\",\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        tab_pt = None\n",
    "\n",
    "    masker = JointMasker(\n",
    "        tab_df=train_df[di.tab_cols],\n",
    "        text_cols=di.text_cols,\n",
    "        cols_to_str_fn=cols_to_str_fn,\n",
    "        tokenizer=tokenizer,\n",
    "        collapse_mask_token=True,\n",
    "        max_samples=max_samples,\n",
    "        tab_partition_tree=tab_pt,\n",
    "    )\n",
    "\n",
    "    explainer = shap.explainers.Partition(model=model.predict, masker=masker)\n",
    "    shap_vals = explainer(x)\n",
    "\n",
    "    output_dir = \"/model/shap_vals/\"\n",
    "    print(f\"Results will be saved @: {output_dir}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    with open(os.path.join(output_dir, f\"{config_type}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_vals, f)\n",
    "\n",
    "    return shap_vals\n",
    "\n",
    "\n",
    "def run_all_text_baseline_shap(\n",
    "    config_type,\n",
    "    test_set_size=100,\n",
    "):\n",
    "    # Shap args\n",
    "    args = ConfigLoader(config_type, \"configs/shap_configs.yaml\")\n",
    "    # Dataset info\n",
    "    di = ConfigLoader(args.dataset, \"configs/dataset_configs.yaml\")\n",
    "    # Data\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.text_model_base, model_max_length=512\n",
    "    )\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=args.my_text_model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=\"cuda:0\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        top_k=None,\n",
    "    )\n",
    "\n",
    "    # Define how to convert all columns to a single string\n",
    "    def cols_to_str_fn(array):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                f\"{col}: {val}\"\n",
    "                for col, val in zip(\n",
    "                    di.categorical_cols + di.numerical_cols + di.text_cols, array\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    np.random.seed(1)\n",
    "    x = list(\n",
    "        map(\n",
    "            cols_to_str_fn,\n",
    "            test_df[di.categorical_cols + di.numerical_cols + di.text_cols].values,\n",
    "        )\n",
    "    )\n",
    "    explainer = shap.Explainer(text_pipeline, tokenizer)\n",
    "    shap_vals = explainer(x)\n",
    "\n",
    "    output_dir = \"models/shap_vals/\"\n",
    "    print(f\"Results will be saved @: {output_dir}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    with open(os.path.join(output_dir, f\"{config_type}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_vals, f)\n",
    "\n",
    "    return shap_vals\n",
    "\n",
    "\n",
    "def load_shap_vals(config_name, add_parent_dir=False):\n",
    "    pre = \"../\" if add_parent_dir else \"\"  # for running from notebooks\n",
    "    with open(f\"{pre}models/shap_vals/{config_name}.pkl\", \"rb\") as f:\n",
    "        shap_vals = pickle.load(f)\n",
    "    return shap_vals\n",
    "\n",
    "\n",
    "def gen_summary_shap_vals(config_type, add_parent_dir=False):\n",
    "    # Shap args\n",
    "    args = ConfigLoader(config_type, \"configs/shap_configs.yaml\")\n",
    "    # Dataset info\n",
    "    di = ConfigLoader(args.dataset, \"configs/dataset_configs.yaml\")\n",
    "    shap_vals = load_shap_vals(config_type, add_parent_dir=add_parent_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.text_model_base, model_max_length=512\n",
    "    )\n",
    "    filepath = f\"models/shap_vals/summed_{config_type}.pkl\"\n",
    "    print(\n",
    "        f\"\"\"\n",
    "            #################\n",
    "            {config_type}\n",
    "            #################\n",
    "            \"\"\"\n",
    "    )\n",
    "    if \"baseline\" not in config_type:\n",
    "        grouped_shap_vals = []\n",
    "        for label in range(len(di.label_names)):\n",
    "            shap_for_label = []\n",
    "            for idx in tqdm(range(len(shap_vals))):\n",
    "                sv = shap_vals[idx, :, label]\n",
    "                text_ft_ends = text_ft_index_ends(\n",
    "                    sv.data[len(di.categorical_cols + di.numerical_cols) :], tokenizer\n",
    "                )\n",
    "                text_ft_ends = [len(di.categorical_cols + di.numerical_cols)] + [\n",
    "                    x + len(di.categorical_cols + di.numerical_cols) + 1\n",
    "                    for x in text_ft_ends\n",
    "                ]\n",
    "                val = np.append(\n",
    "                    sv.values[: len(di.categorical_cols + di.numerical_cols)],\n",
    "                    [\n",
    "                        np.sum(sv.values[text_ft_ends[i] : text_ft_ends[i + 1]])\n",
    "                        for i in range(len(text_ft_ends) - 1)\n",
    "                    ]\n",
    "                    + [np.sum(sv.values[text_ft_ends[-1] :])],\n",
    "                )\n",
    "\n",
    "                shap_for_label.append(val)\n",
    "            grouped_shap_vals.append(np.vstack(shap_for_label))\n",
    "        print(f\"Saving to {filepath}\")\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_shap_vals), f)\n",
    "\n",
    "    else:\n",
    "        col_name_filepath = f\"models/shap_vals/summed_{config_type}_col_names.pkl\"\n",
    "        colon_filepath = f\"models/shap_vals/summed_{config_type}_colons.pkl\"\n",
    "        grouped_shap_vals = []\n",
    "        grouped_col_name_shap_vals = []\n",
    "        grouped_colon_shap_vals = []\n",
    "        for label in range(len(di.label_names)):\n",
    "            shap_for_label = []\n",
    "            shap_for_col_name = []\n",
    "            shap_for_colon = []\n",
    "            for idx in tqdm(range(len(shap_vals))):\n",
    "                sv = shap_vals[idx, :, label]\n",
    "                stripped_data = np.array([item.strip() for item in sv.data])\n",
    "                text_ft_ends = (\n",
    "                    [1] + list(np.where(stripped_data == \"|\")[0]) + [len(sv.data) + 1]\n",
    "                )\n",
    "                # Need this if there are | in the text that aren't col separators\n",
    "                # Not super robust and only implemented for the current col to text\n",
    "                # mapping, but works for now\n",
    "                if (\n",
    "                    len(text_ft_ends)\n",
    "                    != len(di.text_cols + di.categorical_cols + di.numerical_cols) + 1\n",
    "                ):\n",
    "                    text_ft_ends = (\n",
    "                        [1]\n",
    "                        + [\n",
    "                            i\n",
    "                            for i in list(np.where(stripped_data == \"|\")[0])\n",
    "                            if sv.data[i + 1].strip()\n",
    "                            in [\n",
    "                                token_segments(col, tokenizer)[0][1].strip()\n",
    "                                for col in di.categorical_cols\n",
    "                                + di.numerical_cols\n",
    "                                + di.text_cols\n",
    "                            ]\n",
    "                            + di.categorical_cols\n",
    "                            + di.numerical_cols\n",
    "                            + di.text_cols\n",
    "                        ]\n",
    "                        + [len(sv.data) + 1]\n",
    "                    )\n",
    "                assert (\n",
    "                    len(text_ft_ends)\n",
    "                    == len(di.text_cols + di.categorical_cols + di.numerical_cols) + 1\n",
    "                )\n",
    "                val = np.array(\n",
    "                    [\n",
    "                        np.sum(sv.values[text_ft_ends[i] : text_ft_ends[i + 1]])\n",
    "                        for i in range(len(text_ft_ends) - 1)\n",
    "                    ]\n",
    "                )\n",
    "                colon_idxs = np.where(stripped_data == \":\")[0]\n",
    "                col_idxs_after_ft = [\n",
    "                    colon_idxs[list(np.where(colon_idxs > te)[0])[0]]\n",
    "                    for te in text_ft_ends[:-1]\n",
    "                ]\n",
    "                ft_name_vals = np.array(\n",
    "                    [\n",
    "                        np.sum(sv.values[text_ft_ends[i] : col_idxs_after_ft[i]])\n",
    "                        for i in range(len(text_ft_ends) - 1)\n",
    "                    ]\n",
    "                )\n",
    "                colon_vals = np.array(sv.values[col_idxs_after_ft])\n",
    "                shap_for_label.append(val)\n",
    "                shap_for_col_name.append(ft_name_vals)\n",
    "                shap_for_colon.append(colon_vals)\n",
    "            grouped_shap_vals.append(np.vstack(shap_for_label))\n",
    "            grouped_col_name_shap_vals.append(shap_for_col_name)\n",
    "            grouped_colon_shap_vals.append(shap_for_colon)\n",
    "        print(f\"Saving to {filepath}\")\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_shap_vals), f)\n",
    "        print(f\"Saving to {col_name_filepath}\")\n",
    "        with open(col_name_filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_col_name_shap_vals), f)\n",
    "        print(f\"Saving to {colon_filepath}\")\n",
    "        with open(colon_filepath, \"wb\") as f:\n",
    "            pickle.dump(np.array(grouped_colon_shap_vals), f)\n",
    "\n",
    "\n",
    "def load_shap_vals_legacy(\n",
    "    ds_name,\n",
    "    text_model_code,\n",
    "    add_parent_dir=True,\n",
    "    tab_scale_factor=2,\n",
    "    repeat_idx=None,\n",
    "):\n",
    "    pre = \"../\" if add_parent_dir else \"\"  # for running from notebooks\n",
    "    tab_pre = f\"_sf{tab_scale_factor}\" if tab_scale_factor != 2 else \"\"\n",
    "    repeat_idx_str = f\"_{repeat_idx}\" if repeat_idx is not None else \"\"\n",
    "    text_model_name = f\"_{text_model_code}\"\n",
    "\n",
    "\n",
    "    with open(\n",
    "        f\"{pre}models/shap_vals{text_model_name}{tab_pre}{repeat_idx_str}/{ds_name}/shap_vals_all_text.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        shap_all_text = pickle.load(f)\n",
    "    with open(\n",
    "        f\"{pre}models/shap_vals{text_model_name}{tab_pre}{repeat_idx_str}/{ds_name}/shap_vals_all_text_baseline.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        shap_all_text_baseline = pickle.load(f)\n",
    "    return (\n",
    "        [shap_all_text, shap_all_text_baseline],\n",
    "        [\n",
    "            \"all_text\",\n",
    "            \"all_text_baseline\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_type = parser.parse_args().config\n",
    "    \n",
    "    run_all_text_baseline_shap(config_type, test_set_size=1000)\n",
    "    gen_summary_shap_vals(config_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed5b76-00a6-4d97-be8a-082d6bf643e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
