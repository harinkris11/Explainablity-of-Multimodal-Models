# Explainablity-of-Multimodal-Models
This repository explores enhancing multimodal model explainability in pet adoption predictions, focusing on text, tabular, and image data integration. It features the 'Joint Masker' method, addressing SHAP explainer limitations in diverse data contexts.
# Explainability of Multimodal Models

## Summary

In the dynamic field of artificial intelligence, multimodal models have taken center stage for their ability to interpret complex, multi-faceted data across numerous domains like healthcare, autonomous navigation, and content recommendation systems. Our project tackles a significant challenge in this domain: enhancing the explainability of these complex models.
Our Hubrid model architecture ingeniously combines visual features from images, contextual information from text, and patterns from structured data, showcasing the advantage of multimodal integration in improving prediction accuracy. By integrating both tabular and textual data, we use an approach employing a JointMasking strategy for explainability, offering deeper insights into how different data modalities influence model predictions.

## Dataset
We've applied our methodology to the pet finder dataset, providing a multimodal strategy to improve predictions and explainations on pet adoption outcomes.

## Setup

To get started with this project, follow the steps below:

1. **Clone the repository**

```bash
git clone https://github.com/harinkris11/Explainablity-of-Multimodal-Models.git

cd Explainablity-of-Multimodal-Models
pip install -r requirements.txt
```

